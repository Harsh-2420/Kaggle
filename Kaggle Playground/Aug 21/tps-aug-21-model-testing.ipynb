{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Aug 21\n\nThis month, our data consists of 99 feature variables and our target variable is loss. We will first perform some basic EDA to take a better look at this data following which we will start working on our models. ","metadata":{}},{"cell_type":"markdown","source":"## Plan\n\nMoving forward this is the plan we are going to be following. Keep in mind, this is not a concrete plan and I might change it as we move through the notebook. This will show you my process on how I approach these datasets.\n\n- *Memory Reduction*\n- *Sampling to Reduce Training Time*\n- *EDA*\n- *Model Development*\n- *Hyperparameter Tuning*\n- *Feature Importance from top models*\n- *Selecting the best Model/Ensembling*","metadata":{}},{"cell_type":"markdown","source":"## Imports \n\nLet's import some of the libraries we will be using throughout the notebook","metadata":{}},{"cell_type":"code","source":"# Data Import on Kaggle\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing libraries for the metrics\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\n\n# Importing libraries for the model\nimport xgboost as xgb \nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-18T19:31:57.203223Z","iopub.execute_input":"2021-10-18T19:31:57.203492Z","iopub.status.idle":"2021-10-18T19:31:57.215030Z","shell.execute_reply.started":"2021-10-18T19:31:57.203464Z","shell.execute_reply":"2021-10-18T19:31:57.214011Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-aug-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-aug-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:08:36.040512Z","iopub.execute_input":"2021-10-18T19:08:36.040790Z","iopub.status.idle":"2021-10-18T19:08:46.208716Z","shell.execute_reply.started":"2021-10-18T19:08:36.040741Z","shell.execute_reply":"2021-10-18T19:08:46.208005Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Memory Reduction\n\nHere, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. ","metadata":{}},{"cell_type":"code","source":"memory_usage = data.memory_usage(deep=True) / 1024 ** 2\nprint('memory usage of features: \\n', memory_usage.head(7))\nprint('memory usage sum: ',memory_usage.sum())","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:10:35.732352Z","iopub.execute_input":"2021-10-18T19:10:35.732596Z","iopub.status.idle":"2021-10-18T19:10:35.742303Z","shell.execute_reply.started":"2021-10-18T19:10:35.732569Z","shell.execute_reply":"2021-10-18T19:10:35.741315Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\nreduced_df = reduce_memory_usage(data, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:10:42.069318Z","iopub.execute_input":"2021-10-18T19:10:42.069586Z","iopub.status.idle":"2021-10-18T19:10:44.891592Z","shell.execute_reply.started":"2021-10-18T19:10:42.069556Z","shell.execute_reply":"2021-10-18T19:10:44.890835Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"reduced_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:10:44.893171Z","iopub.execute_input":"2021-10-18T19:10:44.893580Z","iopub.status.idle":"2021-10-18T19:10:48.247969Z","shell.execute_reply.started":"2021-10-18T19:10:44.893544Z","shell.execute_reply":"2021-10-18T19:10:48.247221Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Sampling Data\n\nNow that we have reduced the memory usage by over 70%, let's sample the data. We are doing this to reduce the model training time. The sampling would preserve the distributions of each feature while taking only 20% of the entire dataset. We can then perform EDA, modelling, hyperparameter tuning and other steps on this sampled data.\n\nOnce we decide on the model we want to use, we can train the final model on the entire dataset again.","metadata":{}},{"cell_type":"code","source":"sample_df = reduced_df.sample(int(len(reduced_df) * 0.2))\nsample_df.shape\n\nsample_df = sample_df.drop(['id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:13:06.661577Z","iopub.execute_input":"2021-10-18T19:13:06.661863Z","iopub.status.idle":"2021-10-18T19:13:06.712487Z","shell.execute_reply.started":"2021-10-18T19:13:06.661832Z","shell.execute_reply":"2021-10-18T19:13:06.711724Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Let's confirm if the sampling is retaining the feature distributions\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.histplot(\n    data=reduced_df, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n)\nsns.histplot(\n    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n)\n\nplt.legend()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:13:07.730143Z","iopub.execute_input":"2021-10-18T19:13:07.730697Z","iopub.status.idle":"2021-10-18T19:13:08.071348Z","shell.execute_reply.started":"2021-10-18T19:13:07.730664Z","shell.execute_reply":"2021-10-18T19:13:08.070657Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nLet's start looking at any correlations that might exist among the features.\nWe will also be looking at the densities of every feature.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\ncorr = reduced_df.iloc[:,:20].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:14:09.375611Z","iopub.execute_input":"2021-10-18T19:14:09.375980Z","iopub.status.idle":"2021-10-18T19:14:10.158313Z","shell.execute_reply.started":"2021-10-18T19:14:09.375943Z","shell.execute_reply":"2021-10-18T19:14:10.157650Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 50))\nfor i in range(len(sample_df.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(sample_df.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(sample_df[sample_df.columns.tolist()[:100][i]], color = '#1a5d57', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:14:10.160010Z","iopub.execute_input":"2021-10-18T19:14:10.160266Z","iopub.status.idle":"2021-10-18T19:14:52.305306Z","shell.execute_reply.started":"2021-10-18T19:14:10.160230Z","shell.execute_reply":"2021-10-18T19:14:52.304566Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection\n\nIn this section, we will use some statistical methods and regressions to find siginificant features and possible interactions between them that might be important. For this, we will test out ANOVA, linear regression and GAM and see the results we get.\n\nFollowing that, we will start training some basic models on the data to make some predictions and see which ones to move forward with.\nWe will test SVM, XGBoost, LightGBM and Random Forrest.  ","metadata":{}},{"cell_type":"code","source":"# The results from the tests were not useful so I've deleted them.\n# I have kept my code for ANOVA below if you want to refer to it.\n\n\n\n# import statsmodels.api as sm\n# from statsmodels.formula.api import ols\n\n# all_columns = \"+\".join(sample_df.columns[:-1])\n# my_formula = \"loss~\" + all_columns\n\n# mod = ols(formula=my_formula,\n#                 data=sample_df, family=sm.families.Gaussian()).fit()\n                \n# aov_table = sm.stats.anova_lm(mod, typ=2)\n# print(aov_table)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:17:46.046236Z","iopub.execute_input":"2021-10-18T19:17:46.046489Z","iopub.status.idle":"2021-10-18T19:17:46.050550Z","shell.execute_reply.started":"2021-10-18T19:17:46.046461Z","shell.execute_reply":"2021-10-18T19:17:46.049781Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Let's use our sample to split the data into train and test sets","metadata":{}},{"cell_type":"code","source":"x = sample_df.drop(['loss'], axis=1)\ny = sample_df.loss\n\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:20:13.793534Z","iopub.execute_input":"2021-10-18T19:20:13.793897Z","iopub.status.idle":"2021-10-18T19:20:13.820758Z","shell.execute_reply.started":"2021-10-18T19:20:13.793861Z","shell.execute_reply":"2021-10-18T19:20:13.820049Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Scaling\n\nHere we will be scaling the train data to normalise it between 0 and 1. This will not have any effect for most of our models since they are boosting but it is needed for the Support Vector Machine (SVM).","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:20:15.760446Z","iopub.execute_input":"2021-10-18T19:20:15.761225Z","iopub.status.idle":"2021-10-18T19:20:15.824695Z","shell.execute_reply.started":"2021-10-18T19:20:15.761175Z","shell.execute_reply":"2021-10-18T19:20:15.823957Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"x_scaled","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:20:20.742700Z","iopub.execute_input":"2021-10-18T19:20:20.743425Z","iopub.status.idle":"2021-10-18T19:20:20.749881Z","shell.execute_reply.started":"2021-10-18T19:20:20.743388Z","shell.execute_reply":"2021-10-18T19:20:20.748894Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Initial Model Training","metadata":{}},{"cell_type":"code","source":"model_dict = {\n    'Random Forest Regressor': RandomForestRegressor(random_state=0, verbose=100),\n    'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=0, verbose=100),\n    'Support Vector Machine': SVR(),\n    'Decison Tree': DecisionTreeRegressor(random_state=0),\n    'XGB': xgb.XGBRegressor(random_state=0, verbose=100),\n    'Light GBM': lgb.LGBMRegressor(random_state=0, verbose=100)\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\nprediction_list = []\nmetric_scores_list = []\n\nfor model, clf in model_dict.items():\n    clf.fit(x_train, y_train)\n    test_preds = clf.predict(x_test)\n    test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n    \n    train_pred =  clf.predict(x_train)\n    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n    \n    model_list.append(model)\n    train_acc_list.append(train_rmse)\n    test_acc_list.append(test_rmse)  \n    print('{} training'.format(model), 'completed')\n\nresults = pd.DataFrame({\"model\": model_list, \"train_rmse\": train_acc_list, \"test_rmse\": test_acc_list})\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T19:34:33.153443Z","iopub.execute_input":"2021-10-18T19:34:33.153708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial Model Selection\n\nNow that we've trained our first batch of models on default parameters, we can eliminate a few which don't do well.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning\n\nIn this step, we are selecting our XGBoost and LightGBM models to perform Hyperparameter tuning on. We'll start off by using GridSearchCV on both these models with various parameters and selecting the best performing ones based on rmse score.","metadata":{}},{"cell_type":"markdown","source":"### XGBoost\n\nThe parameters we set for grid search were:\n\n- learning_rate: 0.003, 0.008\n- max_depth: 3, 5, 7\n- n_estimators: 500, 1000, 2500\n\nand the top performing parameters were\n- learning_rate: 0.003\n- max_depth: 7\n- n_estimators: 2500\n\nwith an RMSE of (-7.915772914886475)","metadata":{}},{"cell_type":"code","source":"params = {\n                       \"learning_rate\":[0.003, 0.008],\n                       \"subsample\":[0.84],\n                       'booster': ['gbtree'],\n                       'tree_method': ['gpu_hist'],\n 'colsample_bytree':[0.70],\n    'max_depth': [7],\n    'n_estimators': [2500],\n}\n\nxgb_estimator = xgb.XGBRegressor(random_state=42)\ngrid = GridSearchCV(xgb_estimator, param_grid=params, scoring='neg_root_mean_squared_error', cv=5, verbose=100)\nxgb_model = grid.fit(x_scaled, y_train)\n\nprint(xgb_model.best_params_, xgb_model.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-17T18:36:05.15726Z","iopub.execute_input":"2021-10-17T18:36:05.158078Z","iopub.status.idle":"2021-10-17T18:43:11.867489Z","shell.execute_reply.started":"2021-10-17T18:36:05.158027Z","shell.execute_reply":"2021-10-17T18:43:11.866643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = xgb.XGBRegressor(random_state=42, booster='gbtree', colsample_bytree= 0.7, learning_rate= 0.003, max_depth=7, n_estimators=2500, subsample= 0.84, tree_method= 'gpu_hist')\nxgb_model.fit(x_train, y_train)\noof_pred1 = xgb_model.predict(x_test)\noof_pred1 = np.clip(oof_pred1, y.min(), y.max())\n\nprint(f'Mean Error: {np.sqrt(mean_squared_error(y_test, oof_pred1))}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM\n\nThe parameters we set for grid search were:\n\n- learning_rate: 0.003, 0.009\n- max_depth: -1, 3, 5\n- n_estimators: 500, 1000\n- num_leaves: 28, 31, 50, 75\n\nand the top performing parameters were\n- learning_rate: 0.003\n- max_depth: -1\n- n_estimators: 1000,\n- num_leaves: 50\n\nwith an RMSE of (-7.9347)","metadata":{}},{"cell_type":"code","source":"params = {\n    'num_leaves': [50],\n    'learning_rate': [0.003],\n    'max_depth': [-1],\n    'n_estimators': [2500],\n}\n\nlgb_estimator = lgb.LGBMRegressor(random_state=42)\n\ngrid = GridSearchCV(lgb_estimator, param_grid=params, scoring='neg_root_mean_squared_error', cv=5, verbose=100)\nlgb_model = grid.fit(x_scaled, y_train)\n\nprint(lgb_model.best_params_, lgb_model.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-17T18:44:27.252164Z","iopub.execute_input":"2021-10-17T18:44:27.252419Z","iopub.status.idle":"2021-10-17T18:49:41.929047Z","shell.execute_reply.started":"2021-10-17T18:44:27.252392Z","shell.execute_reply":"2021-10-17T18:49:41.928145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_model = lgb.LGBMRegressor(learning_rate=0.003, max_depth=-1, n_estimators=1000, num_leaves=50, random_state=42)\nlgb_model.fit(x_train, y_train)\n\noof_pred1 = lgb_model.predict(x_test)\noof_pred1 = np.clip(oof_pred1, y.min(), y.max())\n\nfrom sklearn.metrics import mean_squared_error\nprint(f'Mean Error: {np.sqrt(mean_squared_error(y_test, oof_pred1))}')","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:45:59.880179Z","iopub.execute_input":"2021-10-18T16:45:59.880893Z","iopub.status.idle":"2021-10-18T16:46:28.146805Z","shell.execute_reply.started":"2021-10-18T16:45:59.880857Z","shell.execute_reply":"2021-10-18T16:46:28.145768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance\n\nLet's take a look at Feature Importance for both our models ","metadata":{}},{"cell_type":"code","source":"a1 = lgb_model.feature_importances_\na2 = xgb_model.feature_importances_\n\naxis_x  = x_train.columns.values\naxis_y1 = minmax_scaling(a1, columns=[0])\naxis_y2 = minmax_scaling(a2, columns=[0])\n\nplt.style.use('seaborn-whitegrid') \nplt.figure(figsize=(16, 6), facecolor='lightgray')\nplt.title(f'XGBoost vs Light GBM Feature Importances', fontsize=12)  \n\nplt.scatter(axis_x, axis_y1, s=20, label='XGBoost') \nplt.scatter(axis_x, axis_y2, s=20, label='Light GBM')\n\nplt.legend(fontsize=12, loc=2)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling and finding the best Fit\n\nNow that we have performed hyperparameter tuning for our two top models, XGB and LGBM, we can start taking a deeper look at them and consider a combination of some models.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = lgb_model.predict(test_data.drop('id', axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:48:01.000845Z","iopub.execute_input":"2021-10-18T16:48:01.00144Z","iopub.status.idle":"2021-10-18T16:48:11.199778Z","shell.execute_reply.started":"2021-10-18T16:48:01.001399Z","shell.execute_reply":"2021-10-18T16:48:11.199167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = pd.DataFrame({'id': test_data['id'], 'loss': final_preds})","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:51:24.773718Z","iopub.execute_input":"2021-10-18T16:51:24.774026Z","iopub.status.idle":"2021-10-18T16:51:24.779166Z","shell.execute_reply.started":"2021-10-18T16:51:24.773968Z","shell.execute_reply":"2021-10-18T16:51:24.778474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission\n\n\nnew_df.to_csv(\"submission1.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T16:53:22.521448Z","iopub.execute_input":"2021-10-18T16:53:22.521702Z","iopub.status.idle":"2021-10-18T16:53:23.007542Z","shell.execute_reply.started":"2021-10-18T16:53:22.521675Z","shell.execute_reply":"2021-10-18T16:53:23.006794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}