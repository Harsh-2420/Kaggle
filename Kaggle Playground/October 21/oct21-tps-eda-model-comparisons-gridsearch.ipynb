{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Oct 21\n\nThis month, our data consists of 284 feature variables and our target variable is binary classification. We will first perform some basic EDA to take a better look at this data following which we will start working on our models. \n\n## Plan\n\nMoving forward this is the plan we are going to be following. Keep in mind, this is not a concrete plan and I might change it as we move through the notebook. This will show you my process on how I approach these datasets.\n\n- *Memory Reduction*\n- *Sampling to Reduce Training Time*\n- *EDA*\n- *Model Development*\n- *Hyperparameter Tuning*\n- *Feature Importance from top models*\n- *Selecting the best Model*","metadata":{}},{"cell_type":"markdown","source":"## Imports \n\nLet's import some of the libraries we will be using throughout the notebook","metadata":{}},{"cell_type":"code","source":"# Data Import on Kaggle\nimport os\nimport time\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing libraries for the metrics\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\n\n# Importing libraries for the model\nimport xgboost as xgb \nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\n                              \n\n# sklearn imports for analysis\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-22T18:02:05.938708Z","iopub.status.idle":"2021-10-22T18:02:05.939526Z","shell.execute_reply.started":"2021-10-22T18:02:05.939133Z","shell.execute_reply":"2021-10-22T18:02:05.939168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:50:07.905828Z","iopub.execute_input":"2021-10-22T16:50:07.906135Z","iopub.status.idle":"2021-10-22T16:51:51.604337Z","shell.execute_reply.started":"2021-10-22T16:50:07.906102Z","shell.execute_reply":"2021-10-22T16:51:51.603193Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Memory Reduction\n\nIf you don't have any issues with memory, you can go ahead and skip this step. \nHere, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. ","metadata":{}},{"cell_type":"code","source":"memory_usage = data.memory_usage(deep=True) / 1024 ** 2\nprint('memory usage of features: \\n', memory_usage.head(7))\nprint('memory usage sum: ',memory_usage.sum())","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:51:51.606613Z","iopub.execute_input":"2021-10-22T16:51:51.607122Z","iopub.status.idle":"2021-10-22T16:51:51.652730Z","shell.execute_reply.started":"2021-10-22T16:51:51.607076Z","shell.execute_reply":"2021-10-22T16:51:51.651931Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\nreduced_df = reduce_memory_usage(data, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:51:51.654046Z","iopub.execute_input":"2021-10-22T16:51:51.654346Z","iopub.status.idle":"2021-10-22T16:53:23.034499Z","shell.execute_reply.started":"2021-10-22T16:51:51.654291Z","shell.execute_reply":"2021-10-22T16:53:23.032389Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"reduced_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:23.039968Z","iopub.execute_input":"2021-10-22T16:53:23.040294Z","iopub.status.idle":"2021-10-22T16:53:56.037449Z","shell.execute_reply.started":"2021-10-22T16:53:23.040239Z","shell.execute_reply":"2021-10-22T16:53:56.036185Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Sampling Data\n\nNow that we have reduced the memory usage by over 70%, let's sample the data. \n\nWhy are we doing this? Well, you don't have to. But if you're like me and own a Macbook Air that can't handle a dataset bigger than 300mb, this might be a good idea.\n\nWe are doing this to reduce the training time when we are selecting a model to work on and performing Hyperparamete Tuning in the future. The sampling would preserve the distributions of each feature while taking only 20% of the entire dataset. We can then perform EDA, modelling, hyperparameter tuning and other steps on this sampled data.\n\nOnce we decide on the model we want to use, we can train the final model on the entire dataset again.","metadata":{}},{"cell_type":"code","source":"sample_df = reduced_df.sample(int(len(reduced_df) * 0.2))\nsample_df.shape\n\nsample_df = sample_df.drop(['id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:56.042226Z","iopub.execute_input":"2021-10-22T16:53:56.044730Z","iopub.status.idle":"2021-10-22T16:53:56.539017Z","shell.execute_reply.started":"2021-10-22T16:53:56.044658Z","shell.execute_reply":"2021-10-22T16:53:56.537898Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Let's confirm if the sampling is retaining the feature distributions\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.histplot(\n    data=reduced_df, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n)\nsns.histplot(\n    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n)\n\nplt.legend()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:56.540853Z","iopub.execute_input":"2021-10-22T16:53:56.541147Z","iopub.status.idle":"2021-10-22T16:53:57.128821Z","shell.execute_reply.started":"2021-10-22T16:53:56.541104Z","shell.execute_reply":"2021-10-22T16:53:57.127622Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nLet's start looking at any correlations that might exist among the features.\nWe will also be looking at the densities of every feature.","metadata":{}},{"cell_type":"code","source":"sample_df","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:57.130396Z","iopub.execute_input":"2021-10-22T16:53:57.130723Z","iopub.status.idle":"2021-10-22T16:53:57.169241Z","shell.execute_reply.started":"2021-10-22T16:53:57.130636Z","shell.execute_reply":"2021-10-22T16:53:57.168109Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Check na values\nprint('Amount of existing NaN values', sample_df.isna().sum())\n\nprint('---------')\n# Target Class Distribution\ntarget_dist = sample_df.target.value_counts()\nprint('Distribution of Target Class \\n',target_dist)\nprint(target_dist[0]/(target_dist[0] + target_dist[1]))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:57.171260Z","iopub.execute_input":"2021-10-22T16:53:57.171575Z","iopub.status.idle":"2021-10-22T16:53:57.371629Z","shell.execute_reply.started":"2021-10-22T16:53:57.171537Z","shell.execute_reply":"2021-10-22T16:53:57.370538Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"There doesn't seem to be any nan values in the data. Also, the target class is split evenly between the two groups","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\ncorr = sample_df.iloc[:,:20].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:57.373580Z","iopub.execute_input":"2021-10-22T16:53:57.373903Z","iopub.status.idle":"2021-10-22T16:53:58.097654Z","shell.execute_reply.started":"2021-10-22T16:53:57.373861Z","shell.execute_reply":"2021-10-22T16:53:58.096733Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Before we look at distributions, we need to split the data into continuous and categorical variables.","metadata":{}},{"cell_type":"code","source":"cat_variables = []\n\nfor column in sample_df.columns:\n    if len(sample_df[column].unique()) < 10:\n        cat_variables.append(column)\nprint(cat_variables)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:58.101283Z","iopub.execute_input":"2021-10-22T16:53:58.102224Z","iopub.status.idle":"2021-10-22T16:53:58.857904Z","shell.execute_reply.started":"2021-10-22T16:53:58.102179Z","shell.execute_reply":"2021-10-22T16:53:58.856463Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"So, the categorical features in the dataset are f22, f43 and all the ones from f242-f284. Let's find the  distributions of the rest using kdeplot and the distributions of these using barplot (compared to target).","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (18, 100))\n\nfor i in range(len(sample_df.columns.tolist()[:241])):\n    if sample_df.columns.tolist()[:241][i] in ['f22', 'f43']: \n        continue\n    else:\n        plt.subplot(25,10,i+1)\n        sns.set_style(\"white\")\n        plt.title(sample_df.columns.tolist()[:241][i], size = 12, fontname = 'monospace')\n        a = sns.kdeplot(sample_df[sample_df.columns.tolist()[:241][i]], color = '#1a5d57', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n        plt.ylabel('')\n        plt.xlabel('')\n        plt.xticks(fontname = 'monospace')\n        plt.yticks([])\n        for j in ['right', 'left', 'top']:\n            a.spines[j].set_visible(False)\n            a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:53:58.859477Z","iopub.execute_input":"2021-10-22T16:53:58.860567Z","iopub.status.idle":"2021-10-22T16:58:29.556916Z","shell.execute_reply.started":"2021-10-22T16:53:58.860505Z","shell.execute_reply":"2021-10-22T16:58:29.555094Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Code from https://www.kaggle.com/craigmthomas/tps-oct-2021-eda\n\ncat_features = [\"f22\", \"f43\"]\ncat_features.extend([\"f{}\".format(x) for x in range(242, 285)])\n\nfig, axs = plt.subplots(11, 4, figsize=(4*4, 11*3), squeeze=False, sharey=True)\n\nptr = 0\nfor row in range(11):\n    for col in range(4):  \n        x = sample_df[[cat_features[ptr], \"target\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\n        sns.barplot(x=cat_features[ptr], y=\"# of Samples\", hue=\"target\", data=x, ax=axs[row][col])\n        plt.xlabel(cat_features[ptr])\n        ptr += 1\n        del(x)\nplt.tight_layout()    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:58:29.558124Z","iopub.execute_input":"2021-10-22T16:58:29.558398Z","iopub.status.idle":"2021-10-22T16:58:38.547461Z","shell.execute_reply.started":"2021-10-22T16:58:29.558364Z","shell.execute_reply":"2021-10-22T16:58:38.546523Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"cont_features = []\nfor x in range(0, 242):\n    if (x != 22) and (x!=43):\n        cont_features.append(\"f{}\".format(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:58:38.549261Z","iopub.execute_input":"2021-10-22T16:58:38.549839Z","iopub.status.idle":"2021-10-22T16:58:38.556694Z","shell.execute_reply.started":"2021-10-22T16:58:38.549800Z","shell.execute_reply":"2021-10-22T16:58:38.555575Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscale = MinMaxScaler()\nsample_df[cont_features]=scale.fit_transform(sample_df[cont_features])\nsample_df[cont_features]= scale.transform(sample_df[cont_features])  \n\nprint('Data scaled using : ', scale)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:58:38.558772Z","iopub.execute_input":"2021-10-22T16:58:38.559482Z","iopub.status.idle":"2021-10-22T16:58:42.532344Z","shell.execute_reply.started":"2021-10-22T16:58:38.559417Z","shell.execute_reply":"2021-10-22T16:58:42.531378Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X = sample_df.drop('target', axis=1)\ny = sample_df.target\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.7, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:58:42.534139Z","iopub.execute_input":"2021-10-22T16:58:42.534609Z","iopub.status.idle":"2021-10-22T16:58:42.854986Z","shell.execute_reply.started":"2021-10-22T16:58:42.534566Z","shell.execute_reply":"2021-10-22T16:58:42.853796Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T16:58:42.856610Z","iopub.execute_input":"2021-10-22T16:58:42.856948Z","iopub.status.idle":"2021-10-22T16:58:42.865440Z","shell.execute_reply.started":"2021-10-22T16:58:42.856904Z","shell.execute_reply":"2021-10-22T16:58:42.864159Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection\n\nHere, we will build a basic model for with no tuning and see the top performers from them. (Kaggle has a time-out on the run time of a notebook because of which I'll comment some of these models out. However, I'll keep the top performing models.)","metadata":{}},{"cell_type":"code","source":"model_dict = {\n    'ADABoost': AdaBoostClassifier(),\n    'CatBoost': CatBoostClassifier(verbose=False),\n    'Light GBM': lgb.LGBMClassifier(random_state=0, verbose=-1),\n#     'XGB': xgb.XGBClassifier(random_state=0, n_estimators=10), \n#     'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=0, verbose=1),\n#     'Logistic Reg': LogisticRegression(random_state=0, max_iter=350, solver='lbfgs'),\n#     'Naive Bayes': GaussianNB(), \n#     'Support Vector Machine': SVC(random_state=0, verbose=1),\n#     'K Nearest Classifier': KNeighborsClassifier(),\n#     'Decison Tree': DecisionTreeClassifier(random_state=0),\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\n\nfor model, clf in model_dict.items():\n    start_time = time.time()\n\n    clf.fit(X_train, y_train)\n    \n    # test results\n    test_pred = clf.predict(X_test)\n    test_acc = roc_auc_score(y_test, test_pred)\n    \n    # train results\n    train_pred =  clf.predict(np.float32(X_train))\n    train_acc = roc_auc_score(y_train, train_pred)\n\n    print(model, 'Model')\n    print('Classification Report \\n',classification_report(y_test, test_pred))\n    print('Confusion Matrix \\n',confusion_matrix(y_test,test_pred))\n    print('Train Accuracy: ', train_acc)\n    print('Test Accuracy: ', test_acc)\n    print(\"\\n Ran in %s seconds\" % (time.time() - start_time))\n    print('--------------------------------')\n    \n    model_list.append(model)\n    train_acc_list.append(train_acc)\n    test_acc_list.append(test_acc)   \n    \n\nresults = pd.DataFrame({\"model\": model_list, \"train_accuracy\": train_acc_list, \"test_acc\": test_acc_list})","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:27:21.370645Z","iopub.execute_input":"2021-10-22T17:27:21.370922Z","iopub.status.idle":"2021-10-22T17:27:56.107287Z","shell.execute_reply.started":"2021-10-22T17:27:21.370895Z","shell.execute_reply":"2021-10-22T17:27:56.106358Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2021-10-22T17:18:03.004449Z","iopub.status.idle":"2021-10-22T17:18:03.005787Z","shell.execute_reply.started":"2021-10-22T17:18:03.005346Z","shell.execute_reply":"2021-10-22T17:18:03.005376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial Model Selection\nNow that we've trained our first batch of models on default parameters, we can eliminate a few which don't do well.\n\nThe CatBoost, AdaBoost and the LightGBM models performed the best so we will keep those and perform hyperparamter tuning on them.","metadata":{}},{"cell_type":"markdown","source":"\n## Hyperparameter Tuning\nIn this step, we are selecting our AdaBoost, CatBoost and LightGBM models to perform Hyperparameter tuning on. \n\nWe'll start off by using GridSearchCV/RandomizedSearchCV on both these models with various parameters and selecting the best performing ones based on score.","metadata":{}},{"cell_type":"code","source":"# Before we start, let's define a function that takes the model as an input, trains it, performs predictions\n# and returns the result\n\ndef train_model(model):\n    start_time = time.time()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    roc_auc = roc_auc_score(y_test, preds)\n    time_taken = \"\\n Ran in %s seconds\" % (time.time() - start_time)\n    return roc_auc, time_taken","metadata":{"execution":{"iopub.status.busy":"2021-10-22T19:23:48.402830Z","iopub.execute_input":"2021-10-22T19:23:48.403610Z","iopub.status.idle":"2021-10-22T19:23:48.410136Z","shell.execute_reply.started":"2021-10-22T19:23:48.403563Z","shell.execute_reply":"2021-10-22T19:23:48.408388Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"\n## LightGBM\n\nThe parameters we set for grid search were:\n- learning_rate: 0.003, 0.009\n- max_depth: -1, 3, 5, 7\n- n_estimators: 500, 1000, 2500\n- num_leaves: 28, 31, 50, 75\n\nand the top performing parameters were\n- learning_rate: 0.003\n- max_depth: -1\n- n_estimators: 1000,\n- num_leaves: 50\n\nwith a score of (0.76)","metadata":{}},{"cell_type":"code","source":"# params = {\n#     'num_leaves': [50, 28, 31, 50, 75],\n#     'learning_rate': [0.003, 0.009],\n#     'max_depth': [-1, 3, 5, 7],\n#     'n_estimators': [500, 1000, 2500],\n# }\n\n# lgb_estimator = lgb.LGBMClassifier(random_state=42)\n\n# grid = GridSearchCV(lgb_estimator, param_grid=params, scoring='roc_auc_ovr', cv=5, verbose=100)\n# lgb_model = grid.fit(X_train, y_train)\n\n# print(lgb_model.best_params_, lgb_model.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T04:30:19.906376Z","iopub.execute_input":"2021-10-22T04:30:19.906915Z","iopub.status.idle":"2021-10-22T05:09:18.802956Z","shell.execute_reply.started":"2021-10-22T04:30:19.906862Z","shell.execute_reply":"2021-10-22T05:09:18.802211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_model = lgb.LGBMClassifier(learning_rate=0.003, max_depth=-1, n_estimators=1000, num_leaves=50, random_state=42, verbose=100)\nlgb_score, time = train_model(lgb_model)\nprint('roc_auc of lgb:', lgb_score,' \\n time taken by lgb:', time, 'seconds')","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:10:05.265868Z","iopub.execute_input":"2021-10-22T05:10:05.266447Z","iopub.status.idle":"2021-10-22T05:10:07.842118Z","shell.execute_reply.started":"2021-10-22T05:10:05.266404Z","shell.execute_reply":"2021-10-22T05:10:07.841208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## CatBoost\n\nTo show you more techniques, I have used RandomizedSearchCV for this model which is an alternative to GridSearchCV. Now that you have code samples of both, you can test them out and select which ones you like.","metadata":{}},{"cell_type":"code","source":"# param_dist = { \"learning_rate\": np.linspace(0,0.2,5),\n#                \"max_depth\": randint(3, 10)}\n               \n# #Instantiate RandomSearchCV object\n# cat_model = CatBoostClassifier(random_state=42, verbose=500)\n# rscv = RandomizedSearchCV(cat_model , param_dist, scoring='roc_auc', cv=3)\n\n# #Fit the model\n# rscv.fit(X_train,y_train)\n\n# # Print the tuned parameters and score\n# print(rscv.best_params_)\n# print(rscv.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-22T18:58:36.776487Z","iopub.execute_input":"2021-10-22T18:58:36.777016Z","iopub.status.idle":"2021-10-22T18:58:36.781211Z","shell.execute_reply.started":"2021-10-22T18:58:36.776983Z","shell.execute_reply":"2021-10-22T18:58:36.780080Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"cat_model = CatBoostClassifier(learning_rate=0.003, max_depth=3, n_estimators=1000, random_state=42, verbose=100)\ncat_model.fit(X_train, y_train)\npreds = cat_model.predict(X_test)\nroc_auc = roc_auc_score(y_test, preds)\n# cat_score, time = train_model(cat_model)\nprint('roc_auc of cat:', roc_auc)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T19:00:50.583353Z","iopub.execute_input":"2021-10-22T19:00:50.583683Z","iopub.status.idle":"2021-10-22T19:03:15.433650Z","shell.execute_reply.started":"2021-10-22T19:00:50.583623Z","shell.execute_reply":"2021-10-22T19:03:15.432456Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"\n## AdaBoost\n","metadata":{}},{"cell_type":"code","source":"ada_model = CatBoostClassifier(random_state=42, verbose=100)\nada_model.fit(X_train, y_train)\npreds = ada_model.predict(X_test)\nroc_auc = roc_auc_score(y_test, preds)\n# ada_score, time = train_model(ada_model)\nprint('\\n roc_auc of ada:', roc_auc)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T19:24:13.063104Z","iopub.execute_input":"2021-10-22T19:24:13.063411Z","iopub.status.idle":"2021-10-22T19:28:01.245505Z","shell.execute_reply.started":"2021-10-22T19:24:13.063380Z","shell.execute_reply":"2021-10-22T19:28:01.243374Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Histogram Gradient Boosting","metadata":{}},{"cell_type":"code","source":"# Code from https://www.kaggle.com/ankitkalauni/tps-21-oct-single-histgbm-0-85651\n\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n\nhist_params = {'l2_regularization': 1.3244040135051264e-10,\n               'early_stopping': 'True',\n               'learning_rate': 0.0366777965884429, \n               'max_iter': 10000, \n               'max_depth': 3, \n               'max_bins': 129, \n               'min_samples_leaf': 13449, \n               'max_leaf_nodes': 68}\n\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_train, y_train)):\n    hgbm_model = HistGradientBoostingClassifier(**hist_params)\n    hgbm_model.fit(X_train, y_train)\n    preds = hgbm_model.predict(X_test)\n#     pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_test, preds)\n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*20)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T19:52:41.453254Z","iopub.execute_input":"2021-10-22T19:52:41.453712Z","iopub.status.idle":"2021-10-22T20:03:47.474655Z","shell.execute_reply.started":"2021-10-22T19:52:41.453650Z","shell.execute_reply":"2021-10-22T20:03:47.472333Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"The Histogram Gradient Boosting Classifier seems to give the best results. Let's train this on the entire dataset and see the results.","metadata":{}},{"cell_type":"code","source":"# Splitting the entire dataset into train and test\n\n\nX = data.drop('target', axis=1)\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.85, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T20:18:21.250662Z","iopub.execute_input":"2021-10-22T20:18:21.251008Z","iopub.status.idle":"2021-10-22T20:18:24.034397Z","shell.execute_reply.started":"2021-10-22T20:18:21.250976Z","shell.execute_reply":"2021-10-22T20:18:24.033495Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"hist_params = {'l2_regularization': 1.3244040135051264e-10,\n               'early_stopping': 'True',\n               'learning_rate': 0.0366777965884429, \n               'max_iter': 10000, \n               'max_depth': 3, \n               'max_bins': 129, \n               'min_samples_leaf': 13449, \n               'max_leaf_nodes': 68}\n\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    hgbm_model = HistGradientBoostingClassifier(**hist_params)\n    hgbm_model.fit(X_train, y_train)\n    preds = hgbm_model.predict(X_test)\n#     pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_test, preds)\n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*20)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T20:18:24.036201Z","iopub.execute_input":"2021-10-22T20:18:24.036491Z","iopub.status.idle":"2021-10-22T20:18:29.113325Z","shell.execute_reply.started":"2021-10-22T20:18:24.036439Z","shell.execute_reply":"2021-10-22T20:18:29.111734Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":" I'll continue testing with other hyperparameters and models to find better results. Stay Tuned!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}