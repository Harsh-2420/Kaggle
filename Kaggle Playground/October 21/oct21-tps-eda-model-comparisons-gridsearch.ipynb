{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Oct 21\n\nThis month, our data consists of 284 feature variables and our target variable is binary classification. We will first perform some basic EDA to take a better look at this data following which we will start working on our models. \n\n## Plan\n\nMoving forward this is the plan we are going to be following. Keep in mind, this is not a concrete plan and I might change it as we move through the notebook. This will show you my process on how I approach these datasets.\n\n- *Memory Reduction*\n- *Sampling to Reduce Training Time*\n- *EDA*\n- *Model Development*\n- *Hyperparameter Tuning*\n- *Feature Importance from top models*\n- *Selecting the best Model*","metadata":{}},{"cell_type":"markdown","source":"## Imports \n\nLet's import some of the libraries we will be using throughout the notebook","metadata":{}},{"cell_type":"code","source":"# Data Import on Kaggle\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing libraries for the metrics\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\n\n# Importing libraries for the model\nimport xgboost as xgb \nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\n\n# metrics and pickle\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-22T02:49:24.133588Z","iopub.execute_input":"2021-10-22T02:49:24.133932Z","iopub.status.idle":"2021-10-22T02:49:26.524699Z","shell.execute_reply.started":"2021-10-22T02:49:24.133848Z","shell.execute_reply":"2021-10-22T02:49:26.523764Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:49:26.526820Z","iopub.execute_input":"2021-10-22T02:49:26.527299Z","iopub.status.idle":"2021-10-22T02:51:05.053632Z","shell.execute_reply.started":"2021-10-22T02:49:26.527258Z","shell.execute_reply":"2021-10-22T02:51:05.052671Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Memory Reduction\n\nIf you don't have any issues with memory, you can go ahead and skip this step. \nHere, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. ","metadata":{}},{"cell_type":"code","source":"memory_usage = data.memory_usage(deep=True) / 1024 ** 2\nprint('memory usage of features: \\n', memory_usage.head(7))\nprint('memory usage sum: ',memory_usage.sum())","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:51:05.055134Z","iopub.execute_input":"2021-10-22T02:51:05.055474Z","iopub.status.idle":"2021-10-22T02:51:05.089254Z","shell.execute_reply.started":"2021-10-22T02:51:05.055430Z","shell.execute_reply":"2021-10-22T02:51:05.088339Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\nreduced_df = reduce_memory_usage(data, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:51:05.092032Z","iopub.execute_input":"2021-10-22T02:51:05.092377Z","iopub.status.idle":"2021-10-22T02:52:17.886561Z","shell.execute_reply.started":"2021-10-22T02:51:05.092335Z","shell.execute_reply":"2021-10-22T02:52:17.885430Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"reduced_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:17.888038Z","iopub.execute_input":"2021-10-22T02:52:17.888266Z","iopub.status.idle":"2021-10-22T02:52:54.657032Z","shell.execute_reply.started":"2021-10-22T02:52:17.888239Z","shell.execute_reply":"2021-10-22T02:52:54.656130Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Sampling Data\n\nNow that we have reduced the memory usage by over 70%, let's sample the data. \n\nWhy are we doing this? Well, you don't have to. But if you're like me and own a Macbook Air that can't handle a dataset bigger than 300mb, this might be a good idea.\n\nWe are doing this to reduce the training time when we are selecting a model to work on and performing Hyperparamete Tuning in the future. The sampling would preserve the distributions of each feature while taking only 20% of the entire dataset. We can then perform EDA, modelling, hyperparameter tuning and other steps on this sampled data.\n\nOnce we decide on the model we want to use, we can train the final model on the entire dataset again.","metadata":{}},{"cell_type":"code","source":"sample_df = reduced_df.sample(int(len(reduced_df) * 0.2))\nsample_df.shape\n\nsample_df = sample_df.drop(['id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:54.658538Z","iopub.execute_input":"2021-10-22T02:52:54.659158Z","iopub.status.idle":"2021-10-22T02:52:55.145275Z","shell.execute_reply.started":"2021-10-22T02:52:54.659114Z","shell.execute_reply":"2021-10-22T02:52:55.144296Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Let's confirm if the sampling is retaining the feature distributions\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.histplot(\n    data=reduced_df, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n)\nsns.histplot(\n    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n)\n\nplt.legend()\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:55.147277Z","iopub.execute_input":"2021-10-22T02:52:55.147543Z","iopub.status.idle":"2021-10-22T02:52:55.626114Z","shell.execute_reply.started":"2021-10-22T02:52:55.147511Z","shell.execute_reply":"2021-10-22T02:52:55.625466Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nLet's start looking at any correlations that might exist among the features.\nWe will also be looking at the densities of every feature.","metadata":{}},{"cell_type":"code","source":"sample_df","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:55.627355Z","iopub.execute_input":"2021-10-22T02:52:55.627763Z","iopub.status.idle":"2021-10-22T02:52:55.666519Z","shell.execute_reply.started":"2021-10-22T02:52:55.627710Z","shell.execute_reply":"2021-10-22T02:52:55.665665Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Check na values\nprint('Amount of existing NaN values', sample_df.isna().sum())\n\nprint('---------')\n# Target Class Distribution\ntarget_dist = sample_df.target.value_counts()\nprint('Distribution of Target Class \\n',target_dist)\nprint(target_dist[0]/(target_dist[0] + target_dist[1]))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:55.667694Z","iopub.execute_input":"2021-10-22T02:52:55.667924Z","iopub.status.idle":"2021-10-22T02:52:55.841190Z","shell.execute_reply.started":"2021-10-22T02:52:55.667895Z","shell.execute_reply":"2021-10-22T02:52:55.840268Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"There doesn't seem to be any nan values in the data. Also, the target class is split evenly between the two groups","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\ncorr = sample_df.iloc[:,:20].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:55.844275Z","iopub.execute_input":"2021-10-22T02:52:55.844651Z","iopub.status.idle":"2021-10-22T02:52:56.524013Z","shell.execute_reply.started":"2021-10-22T02:52:55.844608Z","shell.execute_reply":"2021-10-22T02:52:56.523414Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Before we look at distributions, we need to split the data into continuous and categorical variables.","metadata":{}},{"cell_type":"code","source":"cat_variables = []\n\nfor column in sample_df.columns:\n    if len(sample_df[column].unique()) < 10:\n        cat_variables.append(column)\nprint(cat_variables)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:56.527276Z","iopub.execute_input":"2021-10-22T02:52:56.527644Z","iopub.status.idle":"2021-10-22T02:52:57.567103Z","shell.execute_reply.started":"2021-10-22T02:52:56.527612Z","shell.execute_reply":"2021-10-22T02:52:57.566362Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"So, the categorical features in the dataset are f22, f43 and all the ones from f242-f284. Let's find the  distributions of the rest using kdeplot and the distributions of these using barplot (compared to target).","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (18, 100))\n\nfor i in range(len(sample_df.columns.tolist()[:241])):\n    if sample_df.columns.tolist()[:241][i] in ['f22', 'f43']: \n        continue\n    else:\n        plt.subplot(25,10,i+1)\n        sns.set_style(\"white\")\n        plt.title(sample_df.columns.tolist()[:241][i], size = 12, fontname = 'monospace')\n        a = sns.kdeplot(sample_df[sample_df.columns.tolist()[:241][i]], color = '#1a5d57', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n        plt.ylabel('')\n        plt.xlabel('')\n        plt.xticks(fontname = 'monospace')\n        plt.yticks([])\n        for j in ['right', 'left', 'top']:\n            a.spines[j].set_visible(False)\n            a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T02:52:57.568132Z","iopub.execute_input":"2021-10-22T02:52:57.568865Z","iopub.status.idle":"2021-10-22T02:57:25.771782Z","shell.execute_reply.started":"2021-10-22T02:52:57.568813Z","shell.execute_reply":"2021-10-22T02:57:25.771137Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Code from https://www.kaggle.com/craigmthomas/tps-oct-2021-eda\n\ncat_features = [\"f22\", \"f43\"]\ncat_features.extend([\"f{}\".format(x) for x in range(242, 285)])\n\nfig, axs = plt.subplots(11, 4, figsize=(4*4, 11*3), squeeze=False, sharey=True)\n\nptr = 0\nfor row in range(11):\n    for col in range(4):  \n        x = sample_df[[cat_features[ptr], \"target\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\n        sns.barplot(x=cat_features[ptr], y=\"# of Samples\", hue=\"target\", data=x, ax=axs[row][col])\n        plt.xlabel(cat_features[ptr])\n        ptr += 1\n        del(x)\nplt.tight_layout()    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T03:01:11.841231Z","iopub.execute_input":"2021-10-22T03:01:11.841633Z","iopub.status.idle":"2021-10-22T03:01:19.814803Z","shell.execute_reply.started":"2021-10-22T03:01:11.841586Z","shell.execute_reply":"2021-10-22T03:01:19.814172Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"cont_features = []\nfor x in range(0, 242):\n    if (x != 22) and (x!=43):\n        cont_features.append(\"f{}\".format(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T03:37:45.614280Z","iopub.execute_input":"2021-10-22T03:37:45.614928Z","iopub.status.idle":"2021-10-22T03:37:45.620264Z","shell.execute_reply.started":"2021-10-22T03:37:45.614876Z","shell.execute_reply":"2021-10-22T03:37:45.619270Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscale = MinMaxScaler()\nsample_df[cont_features]=scale.fit_transform(sample_df[cont_features])\nsample_df[cont_features]= scale.transform(sample_df[cont_features])  \n\nprint('Data scaled using : ', scale)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T03:38:00.464066Z","iopub.execute_input":"2021-10-22T03:38:00.464696Z","iopub.status.idle":"2021-10-22T03:38:04.338522Z","shell.execute_reply.started":"2021-10-22T03:38:00.464656Z","shell.execute_reply":"2021-10-22T03:38:04.337467Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"X = sample_df.drop('target', axis=1)\ny = sample_df.target\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.7, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T03:38:08.940156Z","iopub.execute_input":"2021-10-22T03:38:08.940545Z","iopub.status.idle":"2021-10-22T03:38:09.224015Z","shell.execute_reply.started":"2021-10-22T03:38:08.940506Z","shell.execute_reply":"2021-10-22T03:38:09.223043Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T03:38:12.207659Z","iopub.execute_input":"2021-10-22T03:38:12.207972Z","iopub.status.idle":"2021-10-22T03:38:12.214638Z","shell.execute_reply.started":"2021-10-22T03:38:12.207934Z","shell.execute_reply":"2021-10-22T03:38:12.213472Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\n                              \n                              \nmodel_dict = {\n    'ADABoost': AdaBoostClassifier(),\n    'CatBoost': CatBoostClassifier(verbose=0),\n    'Light GBM': lgb.LGBMClassifier(random_state=0, verbose=10),\n    'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=0, verbose=1),\n#     'Logistic Reg': LogisticRegression(random_state=0, max_iter=350, solver='lbfgs'),\n#     'Naive Bayes': GaussianNB(), \n#     'Support Vector Machine': SVC(random_state=0, verbose=1),\n#     'K Nearest Classifier': KNeighborsClassifier(),\n#     'Decison Tree': DecisionTreeClassifier(random_state=0),\n#     'XGB': xgb.XGBClassifier(random_state=0, n_estimators=10)\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\n\nfor model, clf in model_dict.items():\n    clf.fit(X_train, y_train)\n    \n    # test results\n    test_pred = clf.predict(X_test)\n    test_acc = roc_auc_score(y_test, test_pred)\n    \n    # train results\n    train_pred =  clf.predict(np.float32(X_train))\n    train_acc = roc_auc_score(y_train, train_pred)\n\n\n\n    print(model, 'Model')\n    print('Classification Report \\n',classification_report(y_test, test_pred))\n    print('Confusion Matrix \\n',confusion_matrix(y_test,test_pred))\n    print('Train Accuracy: ', train_acc)\n    print('Test Accuracy: ', test_acc)\n    print('--------------------------------')\n    \n    model_list.append(model)\n    train_acc_list.append(train_acc)\n    test_acc_list.append(test_acc)   \n    \n\nresults = pd.DataFrame({\"model\": model_list, \"train_accuracy\": train_acc_list, \"test_acc\": test_acc_list})","metadata":{"execution":{"iopub.status.busy":"2021-10-22T03:43:39.977428Z","iopub.execute_input":"2021-10-22T03:43:39.978325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial Model Selection\nNow that we've trained our first batch of models on default parameters, we can eliminate a few which don't do well.\n\nThe XGBoost and the lightGBM models performed the best so we will keep those and perform hyperparamter tuning on them.\n\n## Hyperparameter Tuning\nIn this step, we are selecting our XGBoost and LightGBM models to perform Hyperparameter tuning on. We'll start off by using GridSearchCV on both these models with various parameters and selecting the best performing ones based on rmse score.\n\n## XGBoost\nThe parameters we set for grid search were:\n\nlearning_rate: 0.003, 0.008\nmax_depth: 3, 5, 7\nn_estimators: 500, 1000, 2500\nand the top performing parameters after gridsearchcv were:\n\nlearning_rate: 0.003\nmax_depth: 7\nn_estimators: 2500\nwith an RMSE of (-7.915772914886475)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}