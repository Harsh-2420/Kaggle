{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Nov 21\n\nThis month, our data consists of 284 feature variables and our target variable is binary classification. In the last two Kaggle Playgrounds I participated in (August and October), I focused on making predictions using ensembling techniques (bagging and boosting). More specifically, I dived into the use of Random Forest, XGBoost, LightGBM and HistGBM. \n\nFor this month, I wanted to take a different approach and use Neural Networks instead. In this notebook, I will use tensorflow (keras) to test and build different models.","metadata":{}},{"cell_type":"markdown","source":"## Imports \n\nLet's import some of the libraries we will be using throughout the notebook","metadata":{}},{"cell_type":"code","source":"# Data Import on Kaggle\nimport os\nimport time\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing libraries for the metrics\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\n\n# Keras Imports\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# sklearn imports for analysis\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom scipy.stats import randint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-04T05:33:44.077276Z","iopub.execute_input":"2021-11-04T05:33:44.077659Z","iopub.status.idle":"2021-11-04T05:33:49.603854Z","shell.execute_reply.started":"2021-11-04T05:33:44.077557Z","shell.execute_reply":"2021-11-04T05:33:49.603116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-nov-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\n\ndata = data.drop('id', axis=1)\ntest_data = test_data.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:33:49.605281Z","iopub.execute_input":"2021-11-04T05:33:49.605947Z","iopub.status.idle":"2021-11-04T05:34:17.317599Z","shell.execute_reply.started":"2021-11-04T05:33:49.605913Z","shell.execute_reply":"2021-11-04T05:34:17.316577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Memory Reduction\n\nIf you don't have any issues with memory, you can go ahead and skip this step. \nHere, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. \n\nThere are several other methods to save RAM - you can refer to this article on [14 tips to save RAM memory](https://www.kaggle.com/pavansanagapati/14-simple-tips-to-save-ram-memory-for-1-gb-dataset). ","metadata":{}},{"cell_type":"code","source":"memory_usage = data.memory_usage(deep=True) / 1024 ** 2\nprint('memory usage of features: \\n', memory_usage.head(7))\nprint('memory usage sum: ',memory_usage.sum())","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:34:17.318925Z","iopub.execute_input":"2021-11-04T05:34:17.319208Z","iopub.status.idle":"2021-11-04T05:34:17.33753Z","shell.execute_reply.started":"2021-11-04T05:34:17.319172Z","shell.execute_reply":"2021-11-04T05:34:17.336749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\ndata = reduce_memory_usage(data, verbose=True)\ntest_data = reduce_memory_usage(test_data, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:34:17.34015Z","iopub.execute_input":"2021-11-04T05:34:17.340438Z","iopub.status.idle":"2021-11-04T05:34:31.7548Z","shell.execute_reply.started":"2021-11-04T05:34:17.340401Z","shell.execute_reply":"2021-11-04T05:34:31.754049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Prep","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX = data.drop('target', axis=1)\ny = data.target\n\ntt = test_data.values\n\nX_scaled = StandardScaler().fit_transform(X)\nX = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n\ntt = StandardScaler().fit_transform(tt)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:40:52.104358Z","iopub.execute_input":"2021-11-04T05:40:52.104653Z","iopub.status.idle":"2021-11-04T05:41:00.043346Z","shell.execute_reply.started":"2021-11-04T05:40:52.104618Z","shell.execute_reply":"2021-11-04T05:41:00.042556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaled_data = data.copy()\n# features = data.columns\n# scale = MinMaxScaler()\n# scaled_data[features]=scale.fit_transform(scaled_data[features])\n# scaled_data[features]= scale.transform(scaled_data[features])  \n\n# test_data_scaled = test_data.copy()\n# test_data_scaled[features.drop('target')]=scale.fit_transform(test_data_scaled[features.drop('target')])\n# test_data_scaled[features.drop('target')]= scale.transform(test_data_scaled[features.drop('target')])  \n\n# X = scaled_data.drop('target', axis=1)\n# y = scaled_data.target\n# tt = test_data_scaled.values","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:41:00.044982Z","iopub.execute_input":"2021-11-04T05:41:00.045895Z","iopub.status.idle":"2021-11-04T05:41:00.05014Z","shell.execute_reply.started":"2021-11-04T05:41:00.045854Z","shell.execute_reply":"2021-11-04T05:41:00.049129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = data.drop('target', axis=1)\n# y = data.target\n\n# tt = test_data.values","metadata":{"execution":{"iopub.status.busy":"2021-11-04T05:41:00.051799Z","iopub.execute_input":"2021-11-04T05:41:00.052103Z","iopub.status.idle":"2021-11-04T05:41:00.063039Z","shell.execute_reply.started":"2021-11-04T05:41:00.052053Z","shell.execute_reply":"2021-11-04T05:41:00.062051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n#     Flatten(input_shape=(100,)),\n    Dense(128, activation=tf.nn.swish),    \n#     Dropout(0.2),\n    Dense(128, activation=tf.nn.swish),\n#     Dropout(0.2),\n    Dense(128, activation=tf.nn.swish),\n#     Dropout(0.2),\n    Dense(64, activation=tf.nn.swish),\n#     Dropout(0.2),\n    Dense(1, activation=tf.nn.sigmoid),\n])","metadata":{"execution":{"iopub.status.busy":"2021-11-04T06:12:55.633042Z","iopub.execute_input":"2021-11-04T06:12:55.63353Z","iopub.status.idle":"2021-11-04T06:12:55.646311Z","shell.execute_reply.started":"2021-11-04T06:12:55.633492Z","shell.execute_reply":"2021-11-04T06:12:55.645611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n# from keras.optimizers import Adam\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')\n\ntest_predictions_nn = np.zeros(test_data.shape[0])\n\nscores_folds = {}\nn_folds = 5\nkf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds['NN'] = []\ncounter = 1\n\n\nfor fold, (trn_ind, val_ind) in enumerate(kf.split(X, y)):\n    print(f'Training fold {fold + 1}')\n    X_train, X_test = X.iloc[trn_ind][:], X.iloc[val_ind][:]\n    y_train, y_test = y.iloc[trn_ind], y.iloc[val_ind]\n    print('CV {}/{}'.format(counter, n_folds)) \n\n\n    model.compile(optimizer='sgd',\n              loss='binary_crossentropy',\n              metrics=['AUC'])\n    model.fit(X_train, \n              y_train, \n              epochs=100, \n              batch_size=1500, \n              validation_data=(X_test, y_test), \n              callbacks=[es, plateau],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n              verbose = 1)\n\n    preds = model.predict(X_test).reshape(1,-1)[0]\n    score = round(roc_auc_score(y_test, preds),5)\n    print('Fold {} {}: {}'.format(counter, 'NN', score))\n    scores_folds['NN'].append(score)\n    test_predictions_nn += model.predict([tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds       \n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2021-11-04T06:12:56.649531Z","iopub.execute_input":"2021-11-04T06:12:56.649795Z","iopub.status.idle":"2021-11-04T06:21:12.253947Z","shell.execute_reply.started":"2021-11-04T06:12:56.649759Z","shell.execute_reply":"2021-11-04T06:21:12.253111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv(\"../input/tabular-playground-series-nov-2021/sample_submission.csv\")\nsub['target']=test_predictions_nn\nsub.to_csv(\"nn_submission11.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T06:28:57.371025Z","iopub.execute_input":"2021-11-04T06:28:57.371597Z","iopub.status.idle":"2021-11-04T06:28:59.24969Z","shell.execute_reply.started":"2021-11-04T06:28:57.371563Z","shell.execute_reply":"2021-11-04T06:28:59.248906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}